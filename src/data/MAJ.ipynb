{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd, sqlite3, requests\r\n",
    "import hockey_scraper as hs\r\n",
    "import numpy as np\r\n",
    "from datetime import date, timedelta, datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "#variables globales\r\n",
    "\r\n",
    "conn = sqlite3.connect('C:/Users/huber/OneDrive/NHL/DbMatchs.db')\r\n",
    "fin = \" WHERE EVENT = 'GOAL' AND Period <> '5' AND PBP.Date >= ALIGNEMENTS.dateDebut AND PBP.Date <= ALIGNEMENTS.dateFin AND PBP.Date >= '2021-01-13' GROUP BY idAlignement)) WHERE idAlignement = ALIGNEMENTS.idAlignement)\"\r\n",
    "\r\n",
    "#scrape les points\r\n",
    "def scrapePoints():\r\n",
    "    ok = \"Scrape fait\"\r\n",
    "    pbpDates = pd.read_sql(\"SELECT MAX(Date) FROM PBP\", conn)\r\n",
    "    maxDate = pbpDates.iloc[0][\"MAX(Date)\"]\r\n",
    "    maxDate = pd.to_datetime(maxDate) + timedelta(days=1)\r\n",
    "    maxDate = maxDate.strftime(\"%Y-%m-%d\")\r\n",
    "    ajd = date.today()\r\n",
    "    ajd = ajd.strftime(\"%Y-%m-%d\")\r\n",
    "\r\n",
    "    try:\r\n",
    "        scrape = hs.nhl.scrape_functions.scrape_date_range(maxDate, ajd,\r\n",
    "                                if_scrape_shifts=True, data_format='pandas', preseason=False, rescrape=False, docs_dir=True)\r\n",
    "        \r\n",
    "        #pbp\r\n",
    "        dfPBP = scrape[\"pbp\"]\r\n",
    "        dfPBP.to_sql('PBP', conn, if_exists='append', index = False)\r\n",
    "\r\n",
    "        #shifts\r\n",
    "        dfshifts = scrape[\"shifts\"]\r\n",
    "        dfshifts.to_sql('SHIFTS', conn, if_exists='append', index = False)\r\n",
    "    except:\r\n",
    "        print(\"PBP à jour\")\r\n",
    "    return ok\r\n",
    "\r\n",
    "#fonctions\r\n",
    "def execSQL(sql, conn, write=False):\r\n",
    "    c = conn.cursor()\r\n",
    "    print(c.execute(sql))\r\n",
    "    if write:\r\n",
    "        conn.commit()\r\n",
    "    else:\r\n",
    "        print(c.fetchall())\r\n",
    "    c.close()\r\n",
    "\r\n",
    "def writeUpdateDebut(typeScore):\r\n",
    "    updateDebut = \"UPDATE ALIGNEMENTS SET \" + typeScore + \"Actuels = (SELECT Nb\" + typeScore + \" FROM ((SELECT idAlignement, COUNT(Event) AS Nb\" + typeScore + \" FROM PBP INNER JOIN ALIGNEMENTS ON\"\r\n",
    "    return updateDebut\r\n",
    "    \r\n",
    "def majInterne(typeScore):\r\n",
    "    debut = writeUpdateDebut(typeScore)\r\n",
    "    if typeScore == \"buts\":\r\n",
    "        milieu = \" (PBP.p1_ID = ALIGNEMENTS.idNHL)\"\r\n",
    "    elif typeScore == \"assist\":\r\n",
    "        milieu = \" (PBP.p2_ID = ALIGNEMENTS.idNHL) OR (PBP.p3_ID = ALIGNEMENTS.idNHL)\"\r\n",
    "    sqlUpdate = debut + milieu + fin\r\n",
    "    execSQL(sqlUpdate, conn, write = True)\r\n",
    "\r\n",
    "def writeSetZero(typeScore):\r\n",
    "    setZero = \"UPDATE ALIGNEMENTS SET \" + typeScore + \"Actuels = 0 WHERE \" + typeScore + \"Actuels IS NULL\"\r\n",
    "    return setZero\r\n",
    "\r\n",
    "def majPointsAlignements():\r\n",
    "    setButsZero = writeSetZero(\"buts\")\r\n",
    "    setAssistZero = writeSetZero(\"assist\")\r\n",
    "    setPtsZero = writeSetZero(\"points\")\r\n",
    "    sqlUpdate = \"UPDATE ALIGNEMENTS SET pointsActuels = (butsActuels + assistActuels)\"\r\n",
    "    execSQL(setButsZero, conn, write = True)\r\n",
    "    execSQL(setAssistZero, conn, write = True)\r\n",
    "    execSQL(setPtsZero, conn, write = True)\r\n",
    "    execSQL(sqlUpdate, conn, write = True)\r\n",
    "\r\n",
    "def majPJ():\r\n",
    "    sqlUpdate = \"UPDATE ALIGNEMENTS SET pjActuels = (SELECT NbPJ FROM (SELECT idAlignement, COUNT(DISTINCT Game_Id) AS NbPJ FROM SHIFTS INNER JOIN ALIGNEMENTS ON (SHIFTS.Player_Id = ALIGNEMENTS.idNHL) AND SHIFTS.Date >= ALIGNEMENTS.dateDebut AND SHIFTS.Date <= ALIGNEMENTS.dateFin AND SHIFTS.Date >= '2021-01-13' GROUP BY idAlignement) WHERE idAlignement = ALIGNEMENTS.idAlignement)\"\r\n",
    "    updateNull = \"UPDATE ALIGNEMENTS SET pjActuels = 0 WHERE pjActuels IS NULL\"\r\n",
    "    execSQL(sqlUpdate, conn, write = True)\r\n",
    "    execSQL(updateNull, conn, write = True)\r\n",
    "\r\n",
    "def majPJBPPAlign():\r\n",
    "    majInterne(\"buts\")\r\n",
    "    majInterne(\"assist\")\r\n",
    "    majPointsAlignements()\r\n",
    "    majPJ()\r\n",
    "\r\n",
    "def majScorePoolers():\r\n",
    "    sqlUpdate = \"UPDATE POOLERS SET PJ = (SELECT NbPJ FROM (SELECT idPooler, SUM(pjActuels) AS NbPJ FROM ALIGNEMENTS WHERE statutJoueur = 'Alignement' GROUP BY idPooler) WHERE Id = idPooler), B = (SELECT NbB FROM (SELECT idPooler, SUM(butsActuels) AS NbB FROM ALIGNEMENTS WHERE statutJoueur = 'Alignement' GROUP BY idPooler) WHERE Id = idPooler), A = (SELECT NbA FROM (SELECT idPooler, SUM(assistActuels) AS NbA FROM ALIGNEMENTS WHERE statutJoueur = 'Alignement' GROUP BY idPooler) WHERE Id = idPooler), Score = (SELECT NbSc FROM (SELECT idPooler, SUM(pointsActuels) AS NbSc FROM ALIGNEMENTS WHERE statutJoueur = 'Alignement' GROUP BY idPooler) WHERE Id = idPooler)\"\r\n",
    "    execSQL(sqlUpdate, conn, write = True)\r\n",
    "    \r\n",
    "#maj du score des gardiens\r\n",
    "def logGardien(idNHL):\r\n",
    "    jsonJoueur = requests.get(\"https://statsapi.web.nhl.com/api/v1/people/\" + idNHL + \"/stats?stats=gameLog&season=20202021\").json()\r\n",
    "    df = pd.json_normalize(jsonJoueur[\"stats\"][0][\"splits\"])\r\n",
    "    gardienLogs = df[[\"season\", \"date\", \"isWin\", \"stat.shutouts\"]]\r\n",
    "    gardienLogs[\"gardien\"] = idNHL\r\n",
    "    return gardienLogs\r\n",
    "\r\n",
    "def GwriteUpdate(typeScore):\r\n",
    "    colAl = \"\"\r\n",
    "    if typeScore == \"isWin\":\r\n",
    "        colAl = \"buts\"\r\n",
    "    elif typeScore == \"'stat.shutouts'\":\r\n",
    "        colAl = \"assist\"\r\n",
    "    elif typeScore == \"scorePool\":\r\n",
    "        colAl = \"points\"\r\n",
    "    debut = \"UPDATE ALIGNEMENTS SET \" + colAl + \"Actuels = (SELECT Nb FROM (SELECT gardien, SUM(\" + typeScore + \") AS Nb FROM GARDIENS WHERE dateMatch >= ALIGNEMENTS.dateDebut AND dateMatch <= ALIGNEMENTS.dateFin AND gardien = ALIGNEMENTS.idNHL)) WHERE ALIGNEMENTS.idNHL IN (SELECT DISTINCT(gardien) FROM GARDIENS)\"\r\n",
    "    return debut\r\n",
    "\r\n",
    "def majScoreGardiens():\r\n",
    "    dfGardiens = pd.read_sql(\"SELECT idNHL FROM JOUEURS WHERE position == 'G'\", conn)\r\n",
    "    gardiens = dfGardiens[\"idNHL\"].apply(str)\r\n",
    "    listGardiens = gardiens.values.tolist()\r\n",
    "\r\n",
    "    vraisCol = [\"season\", \"date\", \"isWin\", \"stat.shutouts\", \"gardien\"]\r\n",
    "    gameLogsG = pd.DataFrame(columns=vraisCol)\r\n",
    "    gameLogsG = gameLogsG.fillna(0)\r\n",
    "\r\n",
    "    for i in listGardiens:\r\n",
    "        try:\r\n",
    "            gardienLogs = logGardien(i)\r\n",
    "        except:\r\n",
    "            print(\"fausse alarme (espoir) \" + i)\r\n",
    "        gameLogsG = gameLogsG.append(gardienLogs)\r\n",
    "    gameLogsG = gameLogsG.rename(columns = {\"date\" : \"dateMatch\"})\r\n",
    "    gameLogsG[[\"dateMatch\"]] = gameLogsG.dateMatch.str.split(\" 00:\",expand=True)\r\n",
    "    #gameLogsG[\"dateMatch\"] = pd.to_datetime(gameLogsG[\"dateMatch\"])\r\n",
    "    pbpDates = pd.read_sql(\"SELECT MAX(Date) FROM PBP\", conn)\r\n",
    "    maxDate = pbpDates.iloc[0][\"MAX(Date)\"]\r\n",
    "    gameLogsG = gameLogsG[(gameLogsG[\"dateMatch\"] <= maxDate)]\r\n",
    "    \r\n",
    "    a = gameLogsG[\"isWin\"]\r\n",
    "    gameLogsG[\"esWin\"] = a * 2\r\n",
    "    gameLogsG [\"scorePool\"] = gameLogsG[\"esWin\"] + gameLogsG[\"stat.shutouts\"]\r\n",
    "    gameLogsG.to_sql(\"GARDIENS\", conn, if_exists=\"replace\", index=False)\r\n",
    "\r\n",
    "    updVic = GwriteUpdate(\"isWin\")\r\n",
    "    updJB = GwriteUpdate(\"'stat.shutouts'\")\r\n",
    "    updSco = GwriteUpdate(\"scorePool\")\r\n",
    "    execSQL(updVic, conn, write = True)\r\n",
    "    execSQL(updJB, conn, write = True)\r\n",
    "    execSQL(updSco, conn, write = True)\r\n",
    "\r\n",
    "#majMasseSalariale()\r\n",
    "def majMasseSalariale():\r\n",
    "    sqlUpdate = \"UPDATE POOLERS SET MasseSalariale = (SELECT masseActuelle FROM (SELECT idPooler, SUM(salaireActuel) AS masseActuelle FROM ALIGNEMENTS INNER JOIN JOUEURS ON ALIGNEMENTS.idNHL = JOUEURS.idNHL WHERE date('now') BETWEEN ALIGNEMENTS.dateDebut AND ALIGNEMENTS.dateFin AND (statutJoueur = 'Alignement' OR statutJoueur = 'Réserve') GROUP BY idPooler) WHERE Poolers.Id = idPooler)\"\r\n",
    "    execSQL(sqlUpdate, conn, write = True)\r\n",
    "\r\n",
    "#maj des json\r\n",
    "def majTables():\r\n",
    "    conn = sqlite3.connect('C:/Users/huber/OneDrive/NHL/DbMatchs.db')\r\n",
    "    var = 'OK'\r\n",
    "    try:\r\n",
    "        #maj Poolers\r\n",
    "        dfPoolers = pd.read_sql(\"SELECT * FROM POOLERS\", conn)\r\n",
    "        jsonPoolers = dfPoolers.to_json(\"C:/dev/pool-de-hockey/src/data/poolers.json\", orient = \"table\", index =    False, indent = 4)\r\n",
    "    except Exception as ex:\r\n",
    "        print(\"Erreur : poolers\")\r\n",
    "        var = \"Erreur poolers\"\r\n",
    "\r\n",
    "    try:\r\n",
    "        #maj Alignements\r\n",
    "        requete = \"SELECT * FROM ALIGNEMENTS INNER JOIN JOUEURS ON JOUEURS.idNHL = ALIGNEMENTS.idNHL\"\r\n",
    "        dfTempo = pd.read_sql(requete, conn)\r\n",
    "        dfAlignements = dfTempo.T.drop_duplicates().T\r\n",
    "        jsonAlignements = dfAlignements.to_json(\"C:/dev/pool-de-hockey/src/data/alignements.json\", orient = \"table\", index = False, indent = 4)\r\n",
    "    except Exception as ex2:\r\n",
    "        print(\"Erreur : alignements\")\r\n",
    "        var = \"Erreur alignements\"\r\n",
    "\r\n",
    "    try:\r\n",
    "        #maj Joueurs\r\n",
    "        dfJoueurs = pd.read_sql(\"SELECT * FROM JOUEURS\", conn)\r\n",
    "        jsonJoueurs = dfJoueurs.to_json(\"C:/dev/pool-de-hockey/src/data/joueurs.json\", orient = \"table\", index = False, indent = 4)\r\n",
    "    except Exception as ex3:\r\n",
    "        print(\"Erreur : joueurs\")\r\n",
    "        var = \"Erreur joueurs\"\r\n",
    "\r\n",
    "    return var\r\n",
    "\r\n",
    "\r\n",
    "def scrape_dates(date_inf, date_sup):\r\n",
    "    ok = \"Scrape fait\"\r\n",
    "    test = pd.date_range(date_inf,date_sup)\r\n",
    "    pbpDates = pd.read_sql(\"SELECT DISTINCT(Date) FROM PBP\", conn)\r\n",
    "    dates = pbpDates[\"Date\"]\r\n",
    "    result = []\r\n",
    "    for i in dates:\r\n",
    "        if i in test:\r\n",
    "            result.append(1)\r\n",
    "        else:\r\n",
    "            result.append(0)\r\n",
    "    if sum(result) > 0:\r\n",
    "        return \"Erreur: tentative de scraper des dates qui sont déjà dans SQLite.\"\r\n",
    "    else:\r\n",
    "        try:\r\n",
    "            scrape = hs.nhl.scrape_functions.scrape_date_range(date_inf, date_sup,\r\n",
    "                                    if_scrape_shifts=True, data_format='pandas', preseason=False, rescrape=False, docs_dir=True)\r\n",
    "\r\n",
    "            #pbp\r\n",
    "            dfPBP = scrape[\"pbp\"]\r\n",
    "            dfPBP.to_sql('PBP', conn, if_exists='append', index = False)\r\n",
    "\r\n",
    "            #shifts\r\n",
    "            dfshifts = scrape[\"shifts\"]\r\n",
    "            dfshifts.to_sql('SHIFTS', conn, if_exists='append', index = False)\r\n",
    "        except:\r\n",
    "            print(\"PBP à jour\")\r\n",
    "        return ok\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#scrapePoints()\r\n",
    "print(\"1\")\r\n",
    "#majPJBPPAlign()\r\n",
    "print(\"2\")\r\n",
    "#majScoreGardiens()\r\n",
    "print(\"3\")\r\n",
    "#majScorePoolers()\r\n",
    "print(\"4\")\r\n",
    "majMasseSalariale()\r\n",
    "print(\"5\")\r\n",
    "majTables()\r\n",
    "print(\"6\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "<sqlite3.Cursor object at 0x000001C20159B1F0>\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "#avant de commencer le scraping, il faut voir si j'ai des matchs en trop\r\n",
    "conn = sqlite3.connect('C:/Users/huber/OneDrive/NHL/DbMatchs.db')\r\n",
    "df_test = pd.read_sql(\"SELECT Game_Id, Date, Event, Away_Team, Home_Team FROM PBP WHERE Event IS 'GEND'\", conn)\r\n",
    "df_test[\"is_AMTL\"] = np.where(df_test[\"Away_Team\"] == \"MTL\",\r\n",
    "                    1, 0)\r\n",
    "df_test[\"is_HMTL\"] = np.where(df_test[\"Home_Team\"] == \"MTL\",\r\n",
    "                    1, 0)\r\n",
    "df_test[\"is_MTL\"] = df_test[\"is_AMTL\"] + df_test[\"is_HMTL\"]                    \r\n",
    "df_test = df_test[df_test[\"is_MTL\"] == 1]\r\n",
    "df_test[['Annee','Mois','Jour']] = df_test['Date'].str.split('-',expand=True)\r\n",
    "df_test[\"Annee\"] = pd.to_numeric(df_test[\"Annee\"])\r\n",
    "df_test[\"Mois\"] = pd.to_numeric(df_test[\"Mois\"])\r\n",
    "df_test[\"saison_series\"] =  df_test['Game_Id'].astype(str).str[0]\r\n",
    "df_test = df_test[df_test[\"saison_series\"] == \"2\"]\r\n",
    "df_test[\"Saison\"] = np.where(df_test[\"Mois\"] > 8,\r\n",
    "                    df_test[\"Annee\"], df_test[\"Annee\"] - 1)\r\n",
    "df_test[\"Saison\"] = df_test[\"Saison\"].map(str)\r\n",
    "df_test[\"Game_Id\"] = df_test[\"Saison\"] + df_test[\"Game_Id\"]\r\n",
    "df_agg = df_test.groupby(['Saison']).count()\r\n",
    "df2020 = df_test[df_test[\"Saison\"] == \"2020\"]\r\n",
    "## Tout semble correct!\r\n",
    "\r\n",
    "#note: df_agg permet de voir le nb de matchs par saison d'une équipe dans la db."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "pbpDates = pd.read_sql(\"SELECT MAX(Date) FROM PBP\", conn)\r\n",
    "maxDate = pbpDates.iloc[0][\"MAX(Date)\"]\r\n",
    "#maxDate = maxDate.strftime(\"%Y-%m-%d\")\r\n",
    "maxDate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-02-28'"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "#scraper le reste de la saison 2020-2021\r\n",
    "#la saison finit le 19 mai 2021\r\n",
    "scrape_dates(\"2021-05-01\", \"2021-05-03\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Scraping Game  2020020796 2021-05-01\n",
      "Scraping Game  2020020801 2021-05-01\n",
      "Scraping Game  2020020797 2021-05-01\n",
      "Scraping Game  2020020798 2021-05-01\n",
      "Scraping Game  2020020799 2021-05-01\n",
      "Scraping Game  2020020800 2021-05-01\n",
      "Scraping Game  2020020805 2021-05-01\n",
      "Scraping Game  2020020806 2021-05-01\n",
      "Scraping Game  2020020808 2021-05-01\n",
      "Scraping Game  2020020207 2021-05-01\n",
      "Scraping Game  2020020802 2021-05-01\n",
      "Scraping Game  2020020804 2021-05-01\n",
      "Scraping Game  2020020809 2021-05-01\n",
      "Scraping Game  2020020810 2021-05-01\n",
      "Scraping Game  2020020807 2021-05-01\n",
      "Scraping Game  2020020811 2021-05-02\n",
      "Scraping Game  2020020020 2021-05-03\n",
      "Scraping Game  2020020812 2021-05-03\n",
      "Scraping Game  2020020813 2021-05-03\n",
      "Scraping Game  2020020814 2021-05-03\n",
      "Scraping Game  2020020815 2021-05-03\n",
      "Scraping Game  2020020816 2021-05-03\n",
      "Scraping Game  2020020817 2021-05-03\n",
      "Scraping Game  2020020819 2021-05-03\n",
      "Scraping Game  2020020820 2021-05-03\n",
      "Scraping Game  2020020818 2021-05-03\n",
      "Scraping Game  2020020821 2021-05-03\n",
      "Scraping Game  2020020822 2021-05-03\n",
      "Scraping Game  2020020829 2021-05-03\n",
      "Scraping Game  2020020823 2021-05-03\n",
      "\n",
      "Players missing IDs:\n",
      "  - NICHOLAS MERKLEY 2020020798\n",
      "  - NICHOLAS MERKLEY 2020020813\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Scrape fait'"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  }
 ]
}